{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GunpowderGuy/lab10/blob/master/Lab_10_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz5h-Uq_lcSF"
      },
      "source": [
        "# **Reinforcement Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkNBUrijleWn"
      },
      "source": [
        "**Equipo:**\n",
        "* Integrante 1 (XX%)\n",
        "* Integrante 2 (XX%)\n",
        "* Integrante 3 (XX%)\n",
        "* Integrante 4 (XX%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhKLL8SuLb_q"
      },
      "source": [
        "### **Objetivo:**\n",
        "\n",
        "Comprender de forma integral el proceso de modelado y resolución de un MDP sencillo (diseñado por ti mismo) mediante el algoritmo de **Value Iteration**, obteniendo la función de valor óptima $V_*(s)$ y la política óptima $\\pi_*$\n",
        "\n",
        "### **Tareas:**\n",
        "\n",
        "1. Recordar la definición formal de un **Proceso de Decisión de Markov (MDP)**.\n",
        "2. Formular un problema realista como un MDP: *estados*, *acciones*, *transiciones* y *recompensas*.\n",
        "3. Implementar el algoritmo de **Value Iteration** para obtener la función de valor óptima $V_*(s)$.\n",
        "4. Derivar una política óptima $\\pi_*$ a partir de $V_*(s)$ y discutir los resultados.\n",
        "\n",
        "### **Entregables**\n",
        "\n",
        "1. **Canvas**: Notebook completo con tu MDP definido, código ejecutado y reflexiones respondidas.\n",
        "2. **Foro**: Publicar su MDP y compartir sus reflexiones.\n",
        "3. Se calificará la **claridad** del modelado, la **correctitud** del algoritmo y la **profundidad** de la discusión (no la complejidad del MDP)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Contexto del problema**\n",
        "\n",
        "En esta práctica **tú** modelarás, programarás y resolverás un MDP a partir de la siguiente narrativa:\n",
        "\n",
        "> Eres el diseñador/a de un **robot mensajero** que se desplaza por un pequeño pasillo de 4 casillas numeradas \\(0,1,2,3\\).  \n",
        "> * El robot **siempre empieza** en la casilla 0.  \n",
        "> * En la casilla 3 hay una estación de entrega que otorga **recompensa +1** y **termina** el episodio.  \n",
        "> * Cada vez que el robot ejecuta una acción que **no** llega a la estación, recibe **recompensa −0.1** (costo de energía).  \n",
        "> * **Acciones disponibles** (en cada estado que no sea terminal):  \n",
        ">   • **\"avanza\"**: intenta moverse +1 casilla.  \n",
        ">   • **\"retrocede\"**: intenta moverse −1 casilla (si ya está en 0 se queda).  \n",
        ">   • **\"espera\"**: permanece en la misma casilla.  \n",
        "> * Debido a fallos de motor, **cada acción tiene un 10 % de probabilidad de no ejecutarse** y deja al robot en la misma casilla (sin recompensa extra).  \n",
        "> * El algoritmo debe usar un **descuento $\\gamma = 0.9$**.\n",
        "\n",
        "Tu **tarea** es **formalizar** esa narrativa como un MDP (listas de `states`, `actions`, `recompensas`, y matriz `P` de transiciones estocásticas) y luego implementar *Value Iteration* para hallar $V_*(s)$ y la política óptima $\\pi_*$."
      ],
      "metadata": {
        "id": "s7YadzQfSHa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Define tu MDP**\n",
        "Abajo encontrarás un esqueleto con TODOs: rellena las listas de estados y acciones, y completa la función `build_P()` que devuelve el diccionario `P` con la estructura:\n",
        "\n",
        "```python\n",
        "P[state][action] = [(probabilidad, estado_siguiente, recompensa), ...]\n",
        "```"
      ],
      "metadata": {
        "id": "3Q1Ityq-StJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ ESTADOS\n",
        "states = # TODO\n",
        "terminal_states = # TODO\n",
        "\n",
        "# 2️⃣ ACCIONES\n",
        "actions = # TODO\n",
        "\n",
        "# 3️⃣ TRANSICIONES\n",
        "\n",
        "def build_P():\n",
        "    \"\"\"Devuelve un diccionario P[s][a] = [(p, s', r), ...].\"\"\"\n",
        "    P = {s: {a: [] for a in actions} for s in states}\n",
        "\n",
        "    # Añade transiciones para *todas* las combinaciones (s,a).\n",
        "\n",
        "    return P\n",
        "\n",
        "P = build_P()\n",
        "\n",
        "# Sanity‑check rápido — muestra una transición cualquiera\n",
        "from pprint import pprint\n",
        "print(\"Ejemplo de P[0][\\\"avanza\\\"] (debería existir):\")\n",
        "pprint(P[0][\"avanza\"])"
      ],
      "metadata": {
        "id": "sM8a9xFpTKY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Algoritmo: Value Iteration\n",
        "Recordemos la ecuación de Bellman para la función de valor óptima:\n",
        "$$\n",
        "    V_*(s) = \\max_{a \\in \\mathcal{A}} \\sum_{s'} p(s'\\,|\\,s,a)\\,\\big[r(s,a,s') + \\gamma \\, V_*(s')\\big].\n",
        "$$\n",
        "El algoritmo itera hasta que el cambio máximo entre dos iteraciones es menor que un umbral $\\theta$."
      ],
      "metadata": {
        "id": "3fXctRh5TbNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "gamma = 0.9   # descuento (mantén 0.9 salvo que quieras experimentar)\n",
        "theta = 1e-6  # tolerancia a la convergencia por defecto\n",
        "\n",
        "# Inicializa V(s) = 0 para todos los estados\n",
        "V = {s: 0.0 for s in states}\n",
        "\n",
        "iteration = 0\n",
        "while True:\n",
        "    delta = 0.0\n",
        "    for s in states:\n",
        "        # TODO\n",
        "    iteration += 1\n",
        "    if delta < theta:\n",
        "        break\n",
        "\n",
        "print(f\"Converged in {iteration} iterations\\n\")\n",
        "print(tabulate([[s, round(V[s],4)] for s in states], headers=[\"Estado\",\"V*(s)\"]))"
      ],
      "metadata": {
        "id": "DALC3p9ZTw1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Derivación de la política óptima**\n",
        "Una vez que cuentas con $V_*(s)$, basta con seleccionar en cada estado la acción que maximiza el retorno esperado:"
      ],
      "metadata": {
        "id": "ZnXMXzSeUiVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(state: int):\n",
        "    if state in terminal_states:\n",
        "        return None\n",
        "    best_a, best_val = None, -np.inf\n",
        "    # TODO: Return the best action\n",
        "\n",
        "policy = {s: greedy_policy(s) for s in states}\n",
        "print(\"Política óptima:\")\n",
        "print(tabulate([[s, policy[s]] for s in states], headers=[\"Estado\",\"π*(s)\"]))"
      ],
      "metadata": {
        "id": "6X6SrEgfUz53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Preguntas para reflexionar**\n",
        "1. **Modelado:** ¿Cómo representa tu MDP la probabilidad de fallo? ¿Por qué asignaste −0.1 como costo por paso?\n",
        "2. **Convergencia:** ¿Cuántas iteraciones tomó? Prueba con $\\gamma = 0.5$ y 0.99, ¿qué varía y por qué?\n",
        "3. **Comparación:** Ajusta la probabilidad de fallo al 20 % y observa cómo cambia $V$ y la política. Explica intuitivamente.\n",
        "4. **Escalabilidad:** ¿Qué obstáculos ves para aplicar Value Iteration cuando el número de casillas crece a 100?"
      ],
      "metadata": {
        "id": "TB1Zer0LVPad"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}